# aws_fully_outomated_ETL_pipeline

Serverless ETL Pipeline with AWS Glue

This project is about building a serverless ETL pipeline on AWS. The idea is simple: whenever a new file gets uploaded to S3, a Glue job runs automatically, processes that file, and then I get notified about the job status.

I used different AWS services together to make this work without managing any servers.

ğŸ”— How the Pipeline Works

Upload data to S3 â€“ Whenever a new file is dropped in the S3 bucket, it kicks off the workflow.

Lambda trigger â€“ A Lambda function listens for the S3 event and starts the Glue job.

Glue ETL job â€“ The Glue job reads the uploaded file, applies transformations using PySpark, and writes the processed output back to another S3 location (or database if needed).

EventBridge â€“ Tracks when the Glue job finishes (success or failure).

SNS notifications â€“ I get an email about whether the ETL job succeeded or failed.

CloudWatch monitoring â€“ Used for logs and debugging.

ğŸ› ï¸ AWS Services Used

S3 â€“ Storage for raw and processed files

Lambda â€“ To trigger Glue jobs automatically

Glue â€“ For the actual ETL process (PySpark script)

EventBridge â€“ To listen for Glue job completion events

SNS â€“ To send me email notifications

CloudWatch â€“ For monitoring and logging

ğŸ“‚ Project Files

lambda_function.py â†’ Lambda function that triggers the Glue job

glue_script.py â†’ PySpark script for data transformation in Glue

project_architecture.png â†’ Architecture diagram (overall flow)

ğŸš€ How to Try This Out

Upload a file into the input S3 bucket.

Lambda will catch the event and run the Glue job.

Glue processes the file and saves the result into the output location.

EventBridge + SNS will send you an email about whether the job passed or failed.

Check CloudWatch logs if something goes wrong.

ğŸ“Š Why This Project

I built this to practice serverless data engineering and see how different AWS services fit together. Itâ€™s cost-effective, scalable, and works without needing to manage any servers.

Some use cases:

Automating data pipelines

Processing raw data for analytics

Getting real-time notifications when data is ready
